{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb65f5c4-2962-45ba-a349-5fff24541bab",
   "metadata": {},
   "source": [
    "# Compute Multilinear regression  - Water and Energy limited "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e163a7da-684c-4d5d-8209-427bdbdddf25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import netCDF4 as nc\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib import cm\n",
    "import cartopy as cart\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.mpl.ticker as cticker\n",
    "from cartopy.util import add_cyclic_point\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import cftime\n",
    "import pandas as pd\n",
    "\n",
    "# Regridding\n",
    "# import xesmf as xe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bd16d5c-d553-416f-91ff-bb9d8de535cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = \"/home/m/m301093/data/clean/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d3f27-7db0-4511-9672-0eac5ab5bbaf",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6de88906-34ef-4296-bf35-34d9dcb9f095",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def xr_fix_time(xr_in, date_start, date_end):\n",
    "    \n",
    "    if xr_in['time'].dt.calendar == 'noleap' or xr_in['time'].dt.calendar == '360_day':\n",
    "        xr_in = xr_in.convert_calendar(calendar = 'gregorian', align_on = 'date')\n",
    "        \n",
    "    else: None\n",
    "    \n",
    "    time = pd.date_range(start=date_start, end=date_end, freq='M').to_numpy(dtype='datetime64[ns]')\n",
    "    xr_out = xr_in.assign_coords(time = time)\n",
    "    \n",
    "    return xr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfd7094c-cb82-4636-aad5-5f822ab4f78b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def xr_clean(xr_in, dims):\n",
    "    data = xr_in.copy()\n",
    "    for i,d in enumerate(dims):\n",
    "        if d in data.coords and d in data.dims:\n",
    "            data = data.drop(d)\n",
    "            data = data.isel({d : 0})\n",
    "        if d in data.coords:\n",
    "            data = data.drop(d)\n",
    "        if d in data.dims and d not in data.coords:\n",
    "            data = data.isel({d : 0})\n",
    "        if data.attrs == {}:\n",
    "            None\n",
    "        else:\n",
    "            if d in data.data_vars:\n",
    "                data = data.drop_vars(d)\n",
    "    xr_out = data\n",
    "    return xr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17d059c3-f1d5-4c44-b3af-50f7b82a6986",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lon180(ds):\n",
    "    ds.coords['lon'] = (ds.coords['lon'] + 180) % 360 - 180\n",
    "    ds = ds.sortby(ds.lon)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "538acbdb-1ace-4752-aae7-d7d487ad3003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import xesmf as xe\n",
    "\n",
    "# def xr_regrid(data, method, lon_bnds, lat_bnds, xres, yres):\n",
    "#     # Regrid XARRAY using xESMF library (https://xesmf.readthedocs.io/en/stable/index.html)\n",
    "#     # data: input data\n",
    "#     # method: \"bilinear\", \"conservative\", \"conservative_normed\", \"patch\", \"nearest_s2d\", \"nearest_d2s\"\n",
    "    \n",
    "#     lonmin = lon_bnds[0]; lonmax = lon_bnds[1]\n",
    "#     latmin = lat_bnds[0]; latmax = lat_bnds[1]\n",
    "#     xr_out = xe.util.cf_grid_2d(lonmin, lonmax, xres, latmin, latmax, yres)\n",
    "   \n",
    "#     regrid = xe.Regridder(data, xr_out, method)\n",
    "\n",
    "#     data_regrid = regrid(data, keep_attrs=True)\n",
    "\n",
    "#     return data_regrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e90b31bd-098d-4670-956f-bb37c673289d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def xr_multimodel_sign(xr_in, models):\n",
    "    # Computes the Multimodel MEAN and STD\n",
    "    # xr_in: list of xarray models\n",
    "    mm = xr.concat(xr_in, dim = \"esm\", coords = \"minimal\", compat = \"override\")\n",
    "    \n",
    "    std = xr.concat(xr_in, dim = \"esm\", coords = \"minimal\", compat = \"override\").std(dim = \"esm\")\n",
    "    \n",
    "    # Multimodel agreement in the sign of the sensitivity\n",
    "    sign = xr.where(mm > 0, 1, -1)\n",
    "    agreement = (np.abs(sign.sum(dim = \"esm\")/len(models)))\n",
    "    \n",
    "    mean = mm.mean(dim = \"esm\")\n",
    "    return (mean, std, agreement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6928e41-350b-4111-aacb-7ffa0a215e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cell_weight(ds):\n",
    "    R = 6.371e6\n",
    "    dϕ = np.deg2rad(ds.lat[1] - ds.lat[0])\n",
    "    dλ = np.deg2rad(ds.lon[1] - ds.lon[0])\n",
    "    dlat = R * dϕ * xr.ones_like(ds.lon)\n",
    "    dlon = R * dλ * np.cos(np.deg2rad(ds.lat))\n",
    "    cell_area = dlon * dlat\n",
    "    return(cell_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36009c44-432d-438b-9b27-bfc9b2532cd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### --------- Mann-Whitney Test --------- ####\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Test applied on a grid-cell basis. For every gid-cell, the statistical difference between two time series is computed.\n",
    "# 1st time series: Nino years' (DS_models_hist_nino)\n",
    "# 2nd time series: Reference climatology (DS_models_hist_clim or DS_models_hist_clim_neutral)  \n",
    "\n",
    "## -- Function for grid-cell operations -- ##\n",
    "def multi_apply_along_axis(func1d, axis, arrs, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Given a function `func1d(A, B, C, ..., *args, **kwargs)`  that acts on \n",
    "    multiple one dimensional arrays, apply that function to the N-dimensional\n",
    "    arrays listed by `arrs` along axis `axis`\n",
    "    \n",
    "    If `arrs` are one dimensional this is equivalent to::\n",
    "    \n",
    "        func1d(*arrs, *args, **kwargs)\n",
    "    \n",
    "    If there is only one array in `arrs` this is equivalent to::\n",
    "    \n",
    "        numpy.apply_along_axis(func1d, axis, arrs[0], *args, **kwargs)\n",
    "        \n",
    "    All arrays in `arrs` must have compatible dimensions to be able to run\n",
    "    `numpy.concatenate(arrs, axis)`\n",
    "    \n",
    "    Arguments:\n",
    "        func1d:   Function that operates on `len(arrs)` 1 dimensional arrays,\n",
    "                  with signature `f(*arrs, *args, **kwargs)`\n",
    "        axis:     Axis of all `arrs` to apply the function along\n",
    "        arrs:     Iterable of numpy arrays\n",
    "        *args:    Passed to func1d after array arguments\n",
    "        **kwargs: Passed to func1d as keyword arguments\n",
    "    \"\"\"\n",
    "    # Concatenate the input arrays along the calculation axis to make one big\n",
    "    # array that can be passed in to `apply_along_axis`\n",
    "    carrs = np.concatenate(arrs, axis)\n",
    "    \n",
    "    # We'll need to split the concatenated arrays up before we apply `func1d`,\n",
    "    # here's the offsets to split them back into the originals\n",
    "    offsets=[]\n",
    "    start=0\n",
    "    for i in range(len(arrs)-1):\n",
    "        start += arrs[i].shape[axis]\n",
    "        offsets.append(start)\n",
    "            \n",
    "    # The helper closure splits up the concatenated array back into the components of `arrs`\n",
    "    # and then runs `func1d` on them\n",
    "    def helperfunc(a, *args, **kwargs):\n",
    "        arrs = np.split(a, offsets)\n",
    "        return func1d(*[*arrs, *args], **kwargs)\n",
    "    \n",
    "    # Run `apply_along_axis` along the concatenated array\n",
    "    return np.apply_along_axis(helperfunc, axis, carrs, *args, **kwargs)\n",
    "\n",
    "\n",
    "def xr_multipletest(p, alpha=0.05, method='fdr_bh', **multipletests_kwargs):\n",
    "    \"\"\"Apply statsmodels.stats.multitest.multipletests for multi-dimensional xr.objects.\"\"\"\n",
    "    from statsmodels.stats.multitest import multipletests\n",
    "    # stack all to 1d array\n",
    "    p_stacked = p.stack(s=p.dims)\n",
    "    # mask only where not nan: https://github.com/statsmodels/statsmodels/issues/2899\n",
    "    mask = np.isfinite(p_stacked)\n",
    "    pvals_corrected = np.full(p_stacked.shape, np.nan)\n",
    "    reject = np.full(p_stacked.shape, np.nan)\n",
    "    # apply test where mask\n",
    "    reject[mask] = multipletests(\n",
    "        p_stacked[mask], alpha=alpha, method=method, **multipletests_kwargs)[0]\n",
    "    pvals_corrected[mask] = multipletests(\n",
    "        p_stacked[mask], alpha=alpha, method=method, **multipletests_kwargs)[1]\n",
    "\n",
    "    def unstack(reject, p_stacked):\n",
    "        \"\"\"Exchange values from p_stacked with reject (1d array) and unstack.\"\"\"\n",
    "        xreject = p_stacked.copy()\n",
    "        xreject.values = reject\n",
    "        xreject = xreject.unstack()\n",
    "        return xreject\n",
    "\n",
    "    reject = unstack(reject, p_stacked)\n",
    "    pvals_corrected = unstack(pvals_corrected, p_stacked)\n",
    "    return reject, pvals_corrected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc395687-10d7-4f69-9a15-f52b32bc9dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lag_linregress_3D(x, y, lagx=0, lagy=0):\n",
    "    \"\"\"\n",
    "    Input: Two xr.Datarrays of any dimensions with the first dim being time. \n",
    "    Thus the input data could be a 1D time series, or for example, have three dimensions (time,lat,lon). \n",
    "    Datasets can be provied in any order, but note that the regression slope and intercept will be calculated\n",
    "    for y with respect to x.\n",
    "    Output: Covariance, correlation, regression slope and intercept, p-value, and standard error on regression\n",
    "    between the two datasets along their aligned time dimension.  \n",
    "    Lag values can be assigned to either of the data, with lagx shifting x, and lagy shifting y, with the specified lag amount. \n",
    "    \"\"\" \n",
    "    #1. Ensure that the data are properly alinged to each other. \n",
    "    x,y = xr.align(x,y)\n",
    "    \n",
    "    #2. Add lag information if any, and shift the data accordingly\n",
    "    if lagx!=0:\n",
    "        #If x lags y by 1, x must be shifted 1 step backwards. \n",
    "        #But as the 'zero-th' value is nonexistant, xr assigns it as invalid (nan). Hence it needs to be dropped\n",
    "        x   = x.shift(time = -lagx).dropna(dim='time')\n",
    "        #Next important step is to re-align the two datasets so that y adjusts to the changed coordinates of x\n",
    "        x,y = xr.align(x,y)\n",
    "\n",
    "    if lagy!=0:\n",
    "        y   = y.shift(time = -lagy).dropna(dim='time')\n",
    "        x,y = xr.align(x,y)\n",
    " \n",
    "    #3. Compute data length, mean and standard deviation along time axis for further use: \n",
    "    n     = x.shape[0]\n",
    "    xmean = x.mean(axis=0)\n",
    "    ymean = y.mean(axis=0)\n",
    "    xstd  = x.std(axis=0)\n",
    "    ystd  = y.std(axis=0)\n",
    "    \n",
    "    #4. Compute covariance along time axis\n",
    "    cov   =  np.sum((x - xmean)*(y - ymean), axis=0)/(n)\n",
    "    \n",
    "    #5. Compute correlation along time axis\n",
    "    cor   = cov/(xstd*ystd)\n",
    "    \n",
    "    #6. Compute regression slope and intercept:\n",
    "    slope     = cov/(xstd**2)\n",
    "    intercept = ymean - xmean*slope\n",
    "    y_pred =  intercept + slope*x\n",
    "    res = y - y_pred\n",
    "\n",
    "    #7. Compute P-value and standard error\n",
    "    #Compute t-statistics\n",
    "    tstats = cor*np.sqrt(n-2)/np.sqrt(1-cor**2)\n",
    "    stderr = slope/tstats\n",
    "    \n",
    "    from scipy.stats import t\n",
    "    pval   = t.sf(tstats, n-2)*2\n",
    "    pval   = xr.DataArray(pval, dims=cor.dims, coords=cor.coords)\n",
    "\n",
    "    #return cov,cor,slope,intercept,pval,stderr\n",
    "    results = {}\n",
    "    results[\"cor\"] = cor\n",
    "    results[\"coef\"] = slope\n",
    "    results[\"pval\"] = pval\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101b9629-3d30-4fa9-89df-3ae8c8855b2a",
   "metadata": {},
   "source": [
    "## Import preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a188177f-7ecc-433a-9875-808b7ad8759d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filepath = glob.glob(os.path.join('/home/m/m301093/data'  +'/'+ \"era5_lai_196001-198912_mean.nc\"))[0]\n",
    "xr_lai_era5 = xr.open_dataset(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"])\n",
    "xr_lai_era5 = xr_lai_era5.isel(time = 0).lai_hv\n",
    "xr_lai_era5 = xr_lai_era5.rename({\"latitude\": \"lat\", \"longitude\": \"lon\"})\n",
    "xr_lai_era5 = lon180(xr_lai_era5)\n",
    "xr_lai_era5 = xr_lai_era5.sel(lat = slice(90,-60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7d4facc-9f6c-47a9-9907-6dad59879a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [ \"ACCESS-ESM1-5\", \"CanESM5\", \"IPSL-CM6A-LR\", \"MPI-ESM1-2-LR\", \"UKESM1-0-LL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "486a9a84-4236-4fdf-b739-92da1ec0a596",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scenario = 'ssp126Lu'\n",
    "xr_aff = []\n",
    "for i,mm in enumerate(models):\n",
    "    filepath = glob.glob(os.path.join(data_path + 'xr_' + scenario + \"_\" + mm + '.nc'))[0]                                       ## List of files sorted by name\n",
    "    content = xr.open_dataset(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"],engine = 'netcdf4',chunks={\"time\": 240})\n",
    "    xr_aff.append(content)\n",
    "\n",
    "\n",
    "scenario = 'ssp370'\n",
    "xr_ctl = []\n",
    "for i,mm in enumerate(models):\n",
    "    filepath = glob.glob(os.path.join(data_path + 'xr_' + scenario + \"_\" + mm + '.nc'))[0]                                       ## List of files sorted by name\n",
    "    content = xr.open_dataset(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"],engine = 'netcdf4',chunks={\"time\": 240})\n",
    "    xr_ctl.append(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9503057-6e1b-4ad5-bf29-83bc4d32f3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = 'ssp126Lu'\n",
    "xr_aff_pft = []\n",
    "for i,mm in enumerate(models):\n",
    "    filepath = glob.glob(os.path.join(data_path + 'xr_' + scenario + \"_\" + mm + '_pft.nc'))[0]                                       ## List of files sorted by name\n",
    "    content = xr.open_dataset(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"],engine = 'netcdf4',chunks={\"time\": 240})\n",
    "    xr_aff_pft.append(content)\n",
    "\n",
    "\n",
    "scenario = 'ssp370'\n",
    "xr_ctl_pft = []\n",
    "for i,mm in enumerate(models):\n",
    "    filepath = glob.glob(os.path.join(data_path + 'xr_' + scenario + \"_\" + mm + '_pft.nc'))[0]                                       ## List of files sorted by name\n",
    "    content = xr.open_dataset(filepath, drop_variables=[\"time_bnds\",\"lon_bnds\",\"lat_bnds\"],engine = 'netcdf4',chunks={\"time\": 240})\n",
    "    xr_ctl_pft.append(content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7163d52-c95f-4aa6-a4b2-ec936a50ebfa",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f48d0122-7f05-4aca-91d1-a91cebaedada",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert pr and evspsbl from kg m-2 s-1 to mm month-1\n",
    "for i,mm in enumerate(models):\n",
    "    xr_ctl[i][\"pr\"] = xr_ctl[i].pr*86400* xr_ctl[i].time.dt.days_in_month\n",
    "    xr_aff[i][\"pr\"] = xr_aff[i].pr*86400* xr_aff[i].time.dt.days_in_month\n",
    "    \n",
    "    xr_ctl[i][\"pr\"].attrs=dict(units=\"mm/months\")\n",
    "    xr_aff[i][\"pr\"].attrs=dict(units=\"mm/months\")\n",
    "\n",
    "    xr_ctl[i][\"evspsbl\"] = xr_ctl[i].evspsbl*86400* xr_ctl[i].time.dt.days_in_month\n",
    "    xr_aff[i][\"evspsbl\"] = xr_aff[i].evspsbl*86400* xr_aff[i].time.dt.days_in_month\n",
    "    \n",
    "    xr_ctl[i][\"evspsbl\"].attrs=dict(units=\"mm/months\")\n",
    "    xr_aff[i][\"evspsbl\"].attrs=dict(units=\"mm/months\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2ba0ee-72ba-492a-80d7-319bdbc813fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### PET estimation according to Priestley-Taylor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f46ca52-7c7f-4c83-8db7-908ee6c5ac3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Cp = 1.0051*10**(-3)   # Specific heat of air at constant pressure (C_p)in MJ/(kg K)\n",
    "Lv = 2.45              # Latent Heat of Vaporization for Water.  FAO uses constant 2.45 MJ/kg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa1f2fef-767b-4396-8916-014b95edb769",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def PriestleyTaylor(xr, alpha=1.26):\n",
    "    # From https://www.nature.com/articles/s41597-023-02290-0\n",
    "    '''Calculate Priestley-Taylor potential evapotranspiration for the given dictionary of input files'''\n",
    "    \n",
    "    # Convert to Celsius if temperature is in Kelvin\n",
    "    # if xr['ts'].attrs['units'] == 'K':\n",
    "    ts=xr['ts']-273.15\n",
    "\n",
    "    # Net radiation = H + LE\n",
    "    Rn = xr.hfss + xr.hfls\n",
    "    \n",
    "    # Psychometric constant\n",
    "    psy = Cp*xr.ps/(1000*Lv*.622)                             # kPa\n",
    "    \n",
    "    # Saturation Vapour Pressure\n",
    "    sat = (0.61078**(17.27*(ts)/(ts+237.3)))            # kPa\n",
    "    \n",
    "    # Delta\n",
    "    delta = 4098*sat/((ts+237.3)**2)                     # kPa/K\n",
    "    \n",
    "    # Calculate PET using the Priestley-Taylor formulation with constant alpha (defalut alpha = 1.26)\n",
    "    xr['pet'] = (alpha*delta*Rn)/(Lv*(delta+psy))             # mm/month\n",
    "\n",
    "    # Mask out the ocean\n",
    "    mask = np.ma.masked_equal(xr.lai, np.nan) \n",
    "    xr['pet'] = xr['pet'] * mask\n",
    "\n",
    "    # Define variable attributes\n",
    "    xr['pet'].attrs['units'] = 'mm month-1'\n",
    "    xr['pet'].attrs['short_name'] = 'PET'\n",
    "    xr['pet'].attrs['long_name'] = 'Priestley-Taylor Potential Evapotranspriation'\n",
    "    xr['pet'].attrs['description'] = 'Priestley-Taylor Potential Evapotranspiration computed using formulation described in Priestley & Taylor (1972) with alpha = '+str(alpha)\n",
    "    \n",
    "    # # Aridity Index\n",
    "    # xr['ai'] = xr['pet']*xr['pr']**-1\n",
    "    # xr['ai'].attrs['units'] = 'mm month-1'\n",
    "    # xr['ai'].attrs['short_name'] = 'AI'\n",
    "    # xr['ai'].attrs['long_name'] = 'Aridity Index'\n",
    "    # xr['ai'].attrs['description'] = 'PET/P'\n",
    "      \n",
    "    return xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e76bb99-add2-4f02-8caa-d8fdda4cec1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, mm in enumerate(models):\n",
    "    PriestleyTaylor(xr_ctl[i], alpha=1.26)\n",
    "    PriestleyTaylor(xr_aff[i], alpha=1.26) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7150ab67-c291-4165-9d29-c1c488c732bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xr_delta = []\n",
    "xr_delta_pft = []\n",
    "for i,mm in enumerate(models):\n",
    "    xr_delta.append(xr_aff[i].sel(time = slice(\"2071-01\",\"2100-12\")).mean(dim = \"time\") - xr_ctl[i].sel(time = slice(\"2071-01\",\"2100-12\")).mean(dim = \"time\"))\n",
    "    xr_delta_pft.append(xr_aff_pft[i].sel(time = slice(\"2071-01\",\"2100-12\")).mean(dim = \"time\") - xr_ctl_pft[i].sel(time = slice(\"2071-01\",\"2100-12\")).mean(dim = \"time\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a836dcd7-3bf7-4eec-9777-98d5d68f822b",
   "metadata": {},
   "source": [
    "#### Water-Energy limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50715f3b-b635-4cb9-a9a8-e33da9446b36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def regime_limit(dataset):\n",
    "    # Definition of water-limited regions as from Forzieri et al., (2020) (https://www.nature.com/articles/s41558-020-0717-0)\n",
    "    # Water-limited: cor(le,pr) > cor(le,ts) and vice-versa for Energy-limited regions\n",
    "    cor_pr = lag_linregress_3D(dataset['hfls'], dataset['pr'])['cor'].compute()\n",
    "    cor_ts = lag_linregress_3D(dataset['hfls'], dataset['ts'])['cor'].compute()\n",
    "    \n",
    "    water_limited = dataset.isel(time = -1).where(cor_pr > cor_ts, drop=True)[\"lai\"]\n",
    "    energy_limited = dataset.isel(time = -1).where(cor_pr < cor_ts, drop=True)[\"lai\"]\n",
    "    \n",
    "    # Ensure computation and time slice selection\n",
    "    return water_limited, energy_limited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ddbcf5b-802f-45ac-8b95-244a34c231e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask_aff_water = []\n",
    "mask_aff_energy = []\n",
    "mask_ctl_water = []\n",
    "mask_ctl_energy = []\n",
    "for i, mm in enumerate(models):\n",
    "    water, energy = regime_limit(xr_aff[i])\n",
    "    mask_aff_water.append(water)\n",
    "    mask_aff_energy.append(energy)\n",
    "    \n",
    "    water, energy = regime_limit(xr_ctl[i])\n",
    "    mask_ctl_water.append(water)\n",
    "    mask_ctl_energy.append(energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00699d31-cbdc-4eb5-a390-c3830c1e4acf",
   "metadata": {},
   "source": [
    "### Mask treeFrac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28914d4d-4fe7-4f3c-b3bc-105835339c00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "dtree = []\n",
    "\n",
    "# masking regions of treefrac increase or decrease\n",
    "mask_treefrac_pos = []\n",
    "mask_treefrac_neg = []\n",
    "\n",
    "for m,_ in enumerate(models):\n",
    "    \n",
    "    # identify regions with 5%+ and 5%- treefrac\n",
    "    mask_treefrac_pos.append(xr_delta_pft[m][\"treeFrac\"].where(xr_delta_pft[m][\"treeFrac\"] > 10))\n",
    "    mask_treefrac_neg.append(xr_delta_pft[m][\"treeFrac\"].where(xr_delta_pft[m][\"treeFrac\"] <- 10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bba37c-ac40-475c-8e28-807f01e171cc",
   "metadata": {},
   "source": [
    "### Effect on PET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ca9170-c8fa-494c-9ba8-543ebf9fc66d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "regions = [\"NA\", \"AFR\", \"SE Asia\", \"SA\", \"EUR\"]\n",
    "\n",
    "boxes = [\n",
    "        (-130, -60, 30, 60),    # NA\n",
    "        (-20, 50, -25, 20),     # Africa\n",
    "        (90, 130, 10, 40),       # SEAsia\n",
    "        (-85, -35, -40, 10),       # SA\n",
    "        (10, 60, 40, 70)       # Eurasia\n",
    "]\n",
    "\n",
    "df_delta_pos = []\n",
    "df_delta_neg = []\n",
    "for i,mm in enumerate(models):\n",
    "    content = {}\n",
    "    content1 = {}\n",
    "    for bb,b in enumerate(boxes):\n",
    "        content[regions[bb]] = ((xr.merge([xr_delta[i],xr_delta_pft[i]]).sel(lon = slice(b[0], b[1]), lat = slice(b[2], b[3]))\n",
    "                              .where(mask_treefrac_pos[i].notnull())\n",
    "                              .stack(cell=[\"lon\", \"lat\"])).compute().dropna(dim='cell')).to_dataframe().drop(columns = [\"lon\", \"lat\",\"height\"])\n",
    "        content1[regions[bb]] = ((xr.merge([xr_delta[i],xr_delta_pft[i]]).sel(lon = slice(b[0], b[1]), lat = slice(b[2], b[3]))\n",
    "                              .where(mask_treefrac_neg[i].notnull())\n",
    "                              .stack(cell=[\"lon\", \"lat\"])).compute().dropna(dim='cell')).to_dataframe().drop(columns = [\"lon\", \"lat\",\"height\"])\n",
    "    \n",
    "    df_delta_pos.append(content)\n",
    "    df_delta_neg.append(content1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03725b85-3494-4a24-bb70-6657abd264a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices = [0,1,3]\n",
    "regions_pos = [regions[i] for i in indices]\n",
    "\n",
    "width_inch = 14\n",
    "height_inch = 6\n",
    "\n",
    "fig = plt.figure(figsize=(width_inch, height_inch)) #, constrained_layout=True)\n",
    "gs = gridspec.GridSpec(3, 5)\n",
    "\n",
    "cmap = \"RdBu\"\n",
    "norm = colors.Normalize(vmin=-3, vmax=3)\n",
    "\n",
    "for m, mm in enumerate(models):\n",
    "    for r, rr in enumerate(regions_pos):\n",
    "        data=df_delta_pos[m][rr]\n",
    "        \n",
    "        axs = fig.add_subplot(gs[r,m])\n",
    "        scatter = axs.scatter(\n",
    "                             x = data[\"treeFrac\"],\n",
    "                             y = data[\"hfss\"],\n",
    "                             c = data[\"pet\"],\n",
    "                             cmap = cmap\n",
    "                            \n",
    "        )\n",
    "        # axs.set_ylim(-1*10**-5, 1*10**-5)\n",
    "        axs.set_xlim(8, 80)\n",
    "        # axs.legend(loc='upper right', fontsize = 10)\n",
    "        axs.set_ylabel(\"\")\n",
    "        if m == 0:\n",
    "            axs.set_ylabel(\"pet\"+ \" - \" + rr)\n",
    "        if r == 2:\n",
    "            axs.set_xlabel(\"treeFrac\")\n",
    "        if r == 0:\n",
    "            axs.set_title(models[m])\n",
    "        # axs.label_outer()\n",
    "        \n",
    "        axs.set_xticklabels(\"\")\n",
    "# fig.tight_layout()\n",
    "cbar_ax = fig.add_axes([0.82, 0.3, 0.02, 0.4])  # Adjust these values to position your colorbar\n",
    "cbar = fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), cax=cbar_ax)\n",
    "cbar.set_label(\"Latent Heat (Wm${-2}$)\")\n",
    "fig.subplots_adjust(bottom=0.2, top=0.95, left=0.1, right=0.8, wspace=0.2, hspace=0.25)\n",
    "fig.suptitle(\"pet - positive treeFrac >10%\", y =1.05, x = 0.45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6776888-273e-44d1-a78e-aa310c51ae38",
   "metadata": {},
   "source": [
    "### Account for Water-Energy limitation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ad5e5b-a757-47da-a4f1-ae25f428470d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "regions = [\"NA\", \"AFR\", \"SE Asia\", \"SA\", \"EUR\"]\n",
    "\n",
    "boxes = [\n",
    "        (-130, -60, 30, 60),    # NA\n",
    "        (-20, 50, -25, 20),     # Africa\n",
    "        (90, 130, 10, 40),       # SEAsia\n",
    "        (-85, -35, -40, 10),       # SA\n",
    "        (10, 60, 40, 70)       # Eurasia\n",
    "]\n",
    "\n",
    "df_delta_pos_water = []\n",
    "df_delta_pos_energy = []\n",
    "for i,mm in enumerate(models):\n",
    "    content = {}\n",
    "    content1 = {}\n",
    "    for bb,b in enumerate(boxes):\n",
    "        content[regions[bb]] = ((xr.merge([xr_delta[i],xr_delta_pft[i]]).sel(lon = slice(b[0], b[1]), lat = slice(b[2], b[3]))\n",
    "                              .where(mask_treefrac_pos[i].notnull()).where(mask_aff_water[i].notnull())\n",
    "                              .stack(cell=[\"lon\", \"lat\"])).compute().dropna(dim='cell')).to_dataframe().drop(columns = [\"lon\", \"lat\",\"height\"])\n",
    "        content1[regions[bb]] = ((xr.merge([xr_delta[i],xr_delta_pft[i]]).sel(lon = slice(b[0], b[1]), lat = slice(b[2], b[3]))\n",
    "                              .where(mask_treefrac_pos[i].notnull()).where(mask_aff_energy[i].notnull())\n",
    "                              .stack(cell=[\"lon\", \"lat\"])).compute().dropna(dim='cell')).to_dataframe().drop(columns = [\"lon\", \"lat\",\"height\"])\n",
    "    \n",
    "    df_delta_pos_water.append(content)\n",
    "    df_delta_pos_energy.append(content1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bf7c4f-5424-4a5e-97cc-d84957b1f379",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_delta_pos_water[4][\"AFR\"].columns #.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af737e9-9b7d-4099-8392-068e6abe9641",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices = [1,3]\n",
    "regions_pos = [regions[i] for i in indices]\n",
    "\n",
    "width_inch = 14\n",
    "height_inch = 6\n",
    "\n",
    "fig = plt.figure(figsize=(width_inch, height_inch)) #, constrained_layout=True)\n",
    "gs = gridspec.GridSpec(2, 5)\n",
    "\n",
    "cmap = \"RdBu\"\n",
    "norm = colors.Normalize(vmin=-3, vmax=3)\n",
    "\n",
    "for m, mm in enumerate(models):\n",
    "    for r, rr in enumerate(regions_pos):\n",
    "        data=df_delta_pos_water[m][rr]\n",
    "        \n",
    "        axs = fig.add_subplot(gs[r,m])\n",
    "        scatter = axs.scatter(\n",
    "                             x = data[\"treeFrac\"],\n",
    "                             y = data[\"hfss\"],\n",
    "                             c = data[\"pet\"],\n",
    "                             cmap = cmap\n",
    "                            \n",
    "        )\n",
    "        # axs.set_ylim(-1*10**-5, 1*10**-5)\n",
    "        # axs.set_xlim(8, 80)\n",
    "        # axs.legend(loc='upper right', fontsize = 10)\n",
    "        axs.set_ylabel(\"\")\n",
    "        if m == 0:\n",
    "            axs.set_ylabel(\"pet\"+ \" - \" + rr)\n",
    "        if r == 2:\n",
    "            axs.set_xlabel(\"treeFrac\")\n",
    "        if r == 0:\n",
    "            axs.set_title(models[m])\n",
    "            axs.set_xticklabels(\"\")\n",
    "        # axs.label_outer()\n",
    "        \n",
    "\n",
    "# fig.tight_layout()\n",
    "cbar_ax = fig.add_axes([0.82, 0.3, 0.02, 0.4])  # Adjust these values to position your colorbar\n",
    "cbar = fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), cax=cbar_ax)\n",
    "cbar.set_label(\"Latent Heat (Wm${-2}$)\")\n",
    "fig.subplots_adjust(bottom=0.2, top=0.95, left=0.1, right=0.8, wspace=0.2, hspace=0.25)\n",
    "fig.suptitle(\"Water limited, treeFrac >10%\", y =1.05, x = 0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94c75c0-a4c0-4564-a72d-fb235181ab85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices = [0,1,3]\n",
    "regions_pos = [regions[i] for i in indices]\n",
    "\n",
    "width_inch = 14\n",
    "height_inch = 8\n",
    "\n",
    "fig = plt.figure(figsize=(width_inch, height_inch)) #, constrained_layout=True)\n",
    "gs = gridspec.GridSpec(3, 5)\n",
    "\n",
    "cmap = \"RdBu\"\n",
    "norm = colors.Normalize(vmin=-3, vmax=3)\n",
    "\n",
    "for m, mm in enumerate(models):\n",
    "    for r, rr in enumerate(regions_pos):\n",
    "        data=df_delta_pos_energy[m][rr]\n",
    "        \n",
    "        axs = fig.add_subplot(gs[r,m])\n",
    "        scatter = axs.scatter(\n",
    "                             x = data[\"treeFrac\"],\n",
    "                             y = data[\"pet\"],\n",
    "                             c = data[\"hfls\"],\n",
    "                             cmap = cmap\n",
    "                            \n",
    "        )\n",
    "        # axs.set_ylim(-1*10**-5, 1*10**-5)\n",
    "        # axs.set_xlim(8, 80)\n",
    "        # axs.legend(loc='upper right', fontsize = 10)\n",
    "        axs.set_ylabel(\"\")\n",
    "        if m == 0:\n",
    "            axs.set_ylabel(\"pet\"+ \" - \" + rr)\n",
    "        if r == 2:\n",
    "            axs.set_xlabel(\"treeFrac\")\n",
    "        if r == 0:\n",
    "            axs.set_title(models[m])\n",
    "            axs.set_xticklabels(\"\")\n",
    "        # axs.label_outer()\n",
    "        \n",
    "\n",
    "# fig.tight_layout()\n",
    "cbar_ax = fig.add_axes([0.82, 0.3, 0.02, 0.4])  # Adjust these values to position your colorbar\n",
    "cbar = fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), cax=cbar_ax)\n",
    "cbar.set_label(\"Latent Heat (Wm${-2}$)\")\n",
    "fig.subplots_adjust(bottom=0.2, top=0.95, left=0.1, right=0.8, wspace=0.2, hspace=0.25)\n",
    "fig.suptitle(\"Energy limited, treeFrac >10%\", y =1.05, x = 0.45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597f1652-82ea-4a9f-8790-2dcd17243412",
   "metadata": {},
   "source": [
    "## Multilinear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a3e348-0e0e-4ef3-ba1e-be372d094194",
   "metadata": {},
   "source": [
    "### Regression with {locator}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a9c23ed-4aa6-4eb2-9fa1-55f28384e8ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detrend_dim(da, dim, degree):\n",
    "    # detrend along a single dimension\n",
    "    p = da.polyfit(dim=dim, deg=degree)\n",
    "    fit = xr.polyval(da[dim], p.polyfit_coefficients)\n",
    "    da_det = (da - fit)\n",
    "    return da_det\n",
    "\n",
    "def pval_calc(x, y, ypred):\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1] \n",
    "\n",
    "    # Calculate residuals\n",
    "    res = y - ypred\n",
    "\n",
    "    # Calculate the standard error of the regression\n",
    "    RSS = np.sum(res**2)\n",
    "    MSE = RSS / (n - p - 1)\n",
    "    # X_with_intercept = np.column_stack((np.ones(n), X))\n",
    "    X_with_intercept = np.append(np.ones((len(X),1)), X, axis=1)\n",
    "\n",
    "    var_b = MSE * np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T @ y\n",
    "\n",
    "    # Standard errors of the coefficients\n",
    "    std_b = np.sqrt(var_b)\n",
    "\n",
    "    # t-statistics for the coefficients\n",
    "    t_stats = coef / std_b[1:]  # Exclude intercept's standard error\n",
    "\n",
    "    # Two-tailed p-values for the t-statistics\n",
    "    p_values = [2 * (1 - stats.t.cdf(np.abs(t), df=n - p - 1)) for t in t_stats]\n",
    "    p_values = np.append(np.nan, p_values)[1:]  # The intercept doesn't have a p-value                                                   \n",
    "    return p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74de1db4-8b7d-49ae-96b2-1fc3fc140e63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "from sklearn import linear_model\n",
    "from regressors import stats\n",
    "\n",
    "linear = linear_model.LinearRegression()\n",
    "# linear = linear_model.Ridge()\n",
    "\n",
    "coef_reg = []\n",
    "pval_reg = []\n",
    "r2_reg = []\n",
    "ypred_reg = []\n",
    "\n",
    "# Define the list of predictors\n",
    "variables = [\"lai\",\"ts\",\"pr\",\"rsds\"]\n",
    "\n",
    "for m, mm in enumerate(models):\n",
    "    data = (xr_aff[m] - xr_ctl[m])\n",
    "    data_pft = (xr_aff_pft[m] - xr_ctl_pft[m])\n",
    "    \n",
    "    ## Preprocessing\n",
    "    # Dimension time on 0th function argument to apply_ufunc with dask='parallelized' consists of multiple chunks, but is also a core dimension. \n",
    "    # To fix, either rechunk into a single dask array chunk along this dimension, i.e., ``.compute()`\n",
    "    # Replace NaN and 0 with 0.01 to avoid Singular Matrix \n",
    "\n",
    "    # X1 = data.lai.resample(time='Y', label='left').mean().compute()\n",
    "\n",
    "    X1 = data_pft.treeFrac.resample(time='Y', label='left').mean().compute()\n",
    "    X2 = data.ts.resample(time='Y', label='left').mean().compute()\n",
    "    X3 = data.pr.resample(time='Y', label='left').sum().compute()\n",
    "    X4 = data.rsds.resample(time='Y', label='left').mean().compute()\n",
    "    Y = data.hfls.resample(time='Y', label='left').mean().compute()\n",
    "    \n",
    "    # 1st order detrending\n",
    "    X1 = detrend_dim(X1, \"time\", 1)\n",
    "    X2 = detrend_dim(X2, \"time\", 1)\n",
    "    X3 = detrend_dim(X3, \"time\", 1)\n",
    "    X4 = detrend_dim(X4, \"time\", 1)\n",
    "    Y = detrend_dim(Y, \"time\", 1)\n",
    "    \n",
    "    ## Standardization\n",
    "    X1 = (X1 - X1.mean(dim = [\"time\"]))/X1.std(dim = [\"time\"])\n",
    "    X2 = (X2 - X2.mean(dim = [\"time\"]))/X2.std(dim = [\"time\"])\n",
    "    X3 = (X3 - X3.mean(dim = [\"time\"]))/X3.std(dim = [\"time\"])\n",
    "    X4 = (X4 - X4.mean(dim = [\"time\"]))/X4.std(dim = [\"time\"])\n",
    "    \n",
    "    # Gap filling\n",
    "    X1 = X1.fillna(0.001); X1 = X1.where(X1 != 0, 0.001)\n",
    "    X2 = X2.fillna(0.001); X2 = X2.where(X2 != 0, 0.001)\n",
    "    X3 = X3.fillna(0.001); X3 = X3.where(X3 != 0, 0.001)\n",
    "    X4 = X4.fillna(0.001); X4 = X4.where(X4 != 0, 0.001)\n",
    "    Y = Y.fillna(0.001); Y = Y.where(Y != 0, 0.001)\n",
    "\n",
    "    # Stack on 1D vectors\n",
    "    X1 = X1.stack(cell = [\"lon\",\"lat\"]); X2 = X2.stack(cell = [\"lon\",\"lat\"]); X3 = X3.stack(cell = [\"lon\",\"lat\"]); X4 = X4.stack(cell = [\"lon\",\"lat\"]); Y = Y.stack(cell = [\"lon\",\"lat\"])\n",
    "\n",
    "    # Create empty dataarray to store regression diagnostics\n",
    "    xr_coef = xr.DataArray(data=None, coords=[X1.cell], dims=[\"cell\"])\n",
    "    xr_pval = xr.DataArray(data=None, coords=[X1.cell], dims=[\"cell\"])\n",
    "    xr_r2 = xr.DataArray(data=None, coords=[X1.cell], dims=[\"cell\"])\n",
    "    xr_ypred = xr.DataArray(data=None, coords=[X1.time,X1.cell], dims=[\"time\",\"cell\"])\n",
    "    for i,vars in enumerate(variables):\n",
    "        xr_coef[vars] = xr.DataArray(data=None, coords=[X1.cell], dims=[\"cell\"])\n",
    "        xr_pval[vars] = xr.DataArray(data=None, coords=[X1.cell], dims=[\"cell\"])\n",
    "\n",
    "        \n",
    "    # Iterate over cells\n",
    "    for c in X1.cell:\n",
    "        locator = {'cell':c}\n",
    "        \n",
    "        X = np.array((X1.loc[locator].values, X2.loc[locator].values, X3.loc[locator].values, X4.loc[locator].values)); X = X.T\n",
    "\n",
    "        # Check if all elements in X are 0.001\n",
    "        # if np.all(X == 0.001):\n",
    "        # Store regression diagnostics\n",
    "        # for i,vars in enumerate([\"lai\",\"ts\",\"pr\",\"rsds\"]):\n",
    "        #     xr_coef[vars].loc[locator] = xr.DataArray(data = np.nan); \n",
    "        #     xr_pval[vars].loc[locator] = xr.DataArray(data = np.nan); \n",
    "        # xr_r2.loc[locator] = xr.DataArray(data = np.nan); \n",
    "        # xr_ypred.loc[locator] =  xr.DataArray(data = np.nan)            \n",
    "        #     continue  # Skip this iteration if all elements are 0.001\n",
    "            \n",
    "        # Regression\n",
    "        model = linear.fit(X, Y.loc[locator].values)\n",
    "        r2 = linear.score(X, Y.loc[locator].values)\n",
    "        ypred = linear.predict(X)\n",
    "        coef = model.coef_\n",
    "        # p_values = pval_calc(X, Y.loc[locator].values, ypred)\n",
    "        p_values = stats.coef_pval(model,X,Y.loc[locator].values)[1:]\n",
    "    \n",
    "        # Store regression diagnostics\n",
    "        for i,vars in enumerate(variables):\n",
    "            xr_coef[vars].loc[locator] = xr.DataArray(data = coef[i]); \n",
    "            xr_pval[vars].loc[locator] = xr.DataArray(data = p_values[i]); \n",
    "        xr_r2.loc[locator] = xr.DataArray(data = r2);\n",
    "        xr_ypred.loc[locator] =  xr.DataArray(data = ypred)\n",
    "     \n",
    "    xr_coef = xr_coef.unstack()\n",
    "    xr_pval = xr_pval.unstack()\n",
    "    xr_r2 = xr_r2.unstack()\n",
    "    xr_ypred = xr_ypred.unstack()\n",
    "    \n",
    "    # Convert object results to np.float64\n",
    "    for i,vars in enumerate(variables):\n",
    "        xr_coef[vars] = xr_coef[vars].astype(np.float64)\n",
    "        xr_pval[vars] = xr_pval[vars].astype(np.float64)\n",
    "    xr_r2 = xr_r2.astype(np.float64)\n",
    "    xr_ypred = xr_ypred.astype(np.float64)\n",
    "    \n",
    "    coef_reg.append(xr_coef)\n",
    "    pval_reg.append(xr_pval)\n",
    "    r2_reg.append(xr_r2)\n",
    "    ypred_reg.append(xr_ypred)\n",
    "    \n",
    "# Save and export regression list data\n",
    "import pickle\n",
    "data_path = '/home/m/m301093/data/'\n",
    "\n",
    "with open(os.path.join(data_path+\"ols_reg_coef_LE_treeFrac\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(coef_reg, fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"ols_reg_pval_LE_treeFrac\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(pval_reg, fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"ols_reg_r2_LE_treeFrac\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(r2_reg, fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"ols_reg_ypred_LE_treeFrac\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(ypred_reg, fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787f4995-4ea2-45a8-958c-d92b79e6aa25",
   "metadata": {},
   "source": [
    "### Regression with {wrapper}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de2cd7be-1be5-43ed-9e19-18f6358d5bf1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detrend_dim(da, dim, degree):\n",
    "    # detrend along a single dimension\n",
    "    p = da.polyfit(dim=dim, deg=degree)\n",
    "    fit = xr.polyval(da[dim], p.polyfit_coefficients)\n",
    "    da_det = (da - fit)\n",
    "    return da_det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3af8112-87e9-4b3f-958e-29f2b4a4867d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "from sklearn import linear_model\n",
    "from regressors import stats\n",
    "\n",
    "linear = linear_model.LinearRegression()\n",
    "# linear = linear_model.Ridge()\n",
    "\n",
    "def xr_MLR(x1, x2, x3, x4, y):\n",
    "      \n",
    "    # Wrapper around scipy linregress to use in apply_ufunc\n",
    "    x1 = x1.reshape(-1,1); x2 = x2.reshape(-1,1); x3 = x3.reshape(-1,1); x4 = x4.reshape(-1,1) #; x5 = x5.reshape(-1,1); x6 = x6.reshape(-1,1)#; x7 = x7.reshape(-1,1)\n",
    "    y = y.reshape(-1,1)\n",
    "    X = np.concatenate((x1, x2, x3, x4), axis = 1)\n",
    "    model = linear.fit(X, y)\n",
    "    r2 = linear.score(X, y)\n",
    "    ypred = linear.predict(X)\n",
    "    coef = model.coef_\n",
    "    pval = stats.coef_pval(model,X,y)[1:] \n",
    "\n",
    "    # pval = stats.coef_pval(model,X,y)[1:]         # First p-value [0] is for intercept\n",
    "    return coef,pval,np.array([r2])\n",
    "\n",
    "coef_reg = []\n",
    "pval_reg = []\n",
    "r2_reg = []\n",
    "for m, mm in enumerate(models):\n",
    "    data = (xr_aff[m] - xr_ctl[m])\n",
    "    data_pft = (xr_aff_pft[m] - xr_ctl_pft[m])\n",
    "    \n",
    "    ## Preprocessing\n",
    "    # Dimension time on 0th function argument to apply_ufunc with dask='parallelized' consists of multiple chunks, but is also a core dimension. \n",
    "    # To fix, either rechunk into a single dask array chunk along this dimension, i.e., ``.compute()`\n",
    "    # Replace NaN and 0 with 0.01 to avoid Singular Matrix \n",
    "    X1 = data.lai.resample(time='Y', label='left').mean().compute(); \n",
    "    X2 = data.ts.resample(time='Y', label='left').mean().compute(); \n",
    "    X3 = data.pr.resample(time='Y', label='left').sum().compute(); \n",
    "    X4 = data.rsds.resample(time='Y', label='left').mean().compute(); \n",
    "    Y = data.hfls.resample(time='Y', label='left').mean().compute(); \n",
    "\n",
    "    ## Standardization\n",
    "    X1 = (X1 - X1.mean(dim = [\"time\"]))/X1.std(dim = [\"time\"])\n",
    "    X2 = (X2 - X2.mean(dim = [\"time\"]))/X2.std(dim = [\"time\"])\n",
    "    X3 = (X3 - X3.mean(dim = [\"time\"]))/X3.std(dim = [\"time\"])\n",
    "    X4 = (X4 - X4.mean(dim = [\"time\"]))/X4.std(dim = [\"time\"])\n",
    "    \n",
    "    # 1st order detrending\n",
    "    X1 = detrend_dim(X1, \"time\", 1)\n",
    "    X2 = detrend_dim(X2, \"time\", 1)\n",
    "    X3 = detrend_dim(X3, \"time\", 1)\n",
    "    X4 = detrend_dim(X4, \"time\", 1)\n",
    "    Y = detrend_dim(Y, \"time\", 1)\n",
    "\n",
    "    X1 = X1.fillna(0.001) #.where(X1 != 0, 0.001)\n",
    "    X2 = X2.fillna(0.001) #.where(X2 != 0, 0.001)\n",
    "    X3 = X3.fillna(0.001) #.where(X3 != 0, 0.001)\n",
    "    X4 = X4.fillna(0.001) #.where(X4 != 0, 0.001)\n",
    "    Y = Y.fillna(0.001) #.where(Y != 0, 0.001)\n",
    "    \n",
    "    # return new DataArrays\n",
    "    xr_coef, xr_pval, xr_r2 = xr.apply_ufunc(xr_MLR,\n",
    "                                            X1, X2, X3, X4, Y,\n",
    "                                            input_core_dims=[['time'],['time'], ['time'], ['time'],['time']],\n",
    "                                            output_core_dims=[['coef'], ['pval'], ['r2']],\n",
    "                                            vectorize=True,\n",
    "                                            dask=\"parallelized\",\n",
    "                                            # allow_rechunk=\"True\",\n",
    "                                            output_dtypes=['float64', 'float64','float64'],\n",
    "                                            output_sizes={\"coef\":4, \"pval\":4, \"r2\":1},\n",
    "                                   )\n",
    "    coef_reg.append(xr_coef)\n",
    "    pval_reg.append(xr_pval)\n",
    "    r2_reg.append(xr_r2)\n",
    "\n",
    "# Save and export regression list data\n",
    "import pickle\n",
    "data_path = '/home/m/m301093/data/'\n",
    "\n",
    "with open(os.path.join(data_path+\"ols_reg_coef_wrap\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(coef_reg, fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"ols_reg_pval_wrap\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(pval_reg, fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"ols_reg_r2_wrap\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(r2_reg, fp)\n",
    "\n",
    "with open(os.path.join(data_path+\"ols_reg_ypred_wrap\"), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(ypred_reg, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bf7caddd-c2bc-42b5-9b84-f4f936a54e21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X1.to_numpy().reshape(-1,1)\n",
    "X = np.concatenate((X1.to_numpy().reshape(-1,1), X2.to_numpy().reshape(-1,1), X3.to_numpy().reshape(-1,1), X4.to_numpy().reshape(-1,1)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1b53534b-5dca-4e0d-ba53-ab70ecda5bf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask = np.all(X == 0.001, axis=1)\n",
    "filtered_data = X[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6b66235f-7c70-43ef-8a2d-9169566de345",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1473696, 4)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0576db7-ea53-4811-9d0a-7704200ae99d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains NaN: False\n"
     ]
    }
   ],
   "source": [
    "# Check if any variable in the Dataset contains NaNs\n",
    "has_nan = X4.isnull().any().item()\n",
    "\n",
    "print(f\"Dataset contains NaN: {has_nan}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b580138a-be47-419d-bee1-037c4f195dbc",
   "metadata": {},
   "source": [
    "### reg on pair of consecutive years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bc78d5-e796-4c63-9f9c-bcda4e0e2d0d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "\n",
    "from sklearn import linear_model\n",
    "from regressors import stats\n",
    "\n",
    "linear = linear_model.LinearRegression()\n",
    "# linear = linear_model.Ridge()\n",
    "\n",
    "def xr_MLR(x1, x2, x3, x4, y):\n",
    "      \n",
    "    # Wrapper around scipy linregress to use in apply_ufunc\n",
    "    x1 = x1.reshape(-1,1); x2 = x2.reshape(-1,1); x3 = x3.reshape(-1,1); x4 = x4.reshape(-1,1) #; x5 = x5.reshape(-1,1); x6 = x6.reshape(-1,1)#; x7 = x7.reshape(-1,1)\n",
    "    y = y.reshape(-1,1)\n",
    "    X = np.concatenate((x1, x2, x3, x4), axis = 1)\n",
    "    model = linear.fit(X, y)\n",
    "    r2 = linear.score(X, y)\n",
    "    ypred = linear.predict(X)\n",
    "    coef = model.coef_\n",
    "    pval = stats.coef_pval(model,X,y)[1:] \n",
    "\n",
    "    # pval = stats.coef_pval(model,X,y)[1:]         # First p-value [0] is for intercept\n",
    "    return coef,pval,np.array([r2])\n",
    "\n",
    "coef_reg = []\n",
    "pval_reg = []\n",
    "r2_reg = []\n",
    "for m, mm in enumerate(models):\n",
    "    data = (xr_aff[m] - xr_ctl[m])\n",
    "    data_pft = (xr_aff_pft[m] - xr_ctl_pft[m])\n",
    "    \n",
    "    # Dimension time on 0th function argument to apply_ufunc with dask='parallelized' consists of multiple chunks, but is also a core dimension. \n",
    "    # To fix, either rechunk into a single dask array chunk along this dimension, i.e., ``.compute()`\n",
    "    # Replace NaN and 0 with 0.01 to avoid Singular Matrix \n",
    "    X1 = data.lai.resample(time='Y', label='left').mean().compute()\n",
    "    X2 = data.ts.resample(time='Y', label='left').mean().compute()\n",
    "    X3 = data.pr.resample(time='Y', label='left').sum().compute()\n",
    "    X4 = data.rsds.resample(time='Y', label='left').mean().compute()\n",
    "    Y = data.hfls.resample(time='Y', label='left').mean().compute()\n",
    "    \n",
    "    X1 = X1.fillna(0.001); X1 = X1.where(X1 != 0, 0.001)\n",
    "    X2 = X2.fillna(0.001); X2 = X2.where(X2 != 0, 0.001)\n",
    "    X3 = X3.fillna(0.001); X3 = X3.where(X3 != 0, 0.001)\n",
    "    X4 = X4.fillna(0.001); X4 = X4.where(X4 != 0, 0.001)\n",
    "    Y = Y.fillna(0.001); Y = Y.where(Y != 0, 0.001)\n",
    "    ## Preprocessing\n",
    "    # Standardization\n",
    "    X1 = X1 - X1.mean(dim = [\"time\"])/X1.std(dim = [\"time\"])\n",
    "    X2 = X2 - X2.mean(dim = [\"time\"])/X2.std(dim = [\"time\"])\n",
    "    X3 = X3 - X3.mean(dim = [\"time\"])/X3.std(dim = [\"time\"])\n",
    "    X4 = X4 - X4.mean(dim = [\"time\"])/X4.std(dim = [\"time\"])\n",
    "\n",
    "    \n",
    "    # Compute the regression for every pair of consecutive years\n",
    "    years = np.arange(0,X1.time.shape[0]-1,1)\n",
    "    coef_m = []; pval_m = []; r2_m = []\n",
    "    for t in years:\n",
    "        X1 = X1.isel(time = [t,t+1]); X2 = X2.isel(time = [t,t+1]); X3 = X3.isel(time = [t,t+1]); X4 = X4.isel(time = [t,t+1])\n",
    "        Y = Y.isel(time = [t,t+1])\n",
    "        # return new DataArrays\n",
    "        coef, pval, r2 = xr.apply_ufunc(xr_MLR,\n",
    "                                                X1, X2, X3, X4, Y,\n",
    "                                                input_core_dims=[['time'],['time'], ['time'], ['time'],['time']],\n",
    "                                                output_core_dims=[['coef'], ['pval'], ['r2']],\n",
    "                                                vectorize=True,\n",
    "                                                dask=\"parallelized\",\n",
    "                                                # allow_rechunk=\"True\",\n",
    "                                                output_dtypes=['float64', 'float64', 'float64'],\n",
    "                                                output_sizes={\"coef\":4, \"pval\":4, \"r2\":4},\n",
    "                                       )\n",
    "        coef_m.append(coef); pval_m.append(pval); r2_m.append(r2)\n",
    "    coef_m = xr.concat(coef_m,dim = \"delta\").mean(dim = \"delta\")\n",
    "    pval_m = xr.concat(pval_m,dim = \"delta\").mean(dim = \"delta\")\n",
    "    r2_m = xr.concat(r2_m,dim = \"delta\").mean(dim = \"delta\")\n",
    "    \n",
    "    coef_reg.append(coef)\n",
    "    pval_reg.append(pval)\n",
    "    r2_reg.append(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40730292-3bee-4b1b-815a-9d21239c76ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from regressors import stats\n",
    "\n",
    "linear = linear_model.LinearRegression()\n",
    "# linear = linear_model.Ridge()\n",
    "\n",
    "coef_reg = []\n",
    "pval_reg = []\n",
    "r2_reg = []\n",
    "\n",
    "## For every ESMs\n",
    "for m, mm in enumerate(models):\n",
    "    data = (xr_aff[m] - xr_ctl[m])\n",
    "    data_pft = (xr_aff_pft[m] - xr_ctl_pft[m])\n",
    "    \n",
    "    # Dimension time on 0th function argument to apply_ufunc with dask='parallelized' consists of multiple chunks, but is also a core dimension. \n",
    "    # To fix, either rechunk into a single dask array chunk along this dimension, i.e., ``.compute()`\n",
    "    # Replace NaN and 0 with 0.01 to avoid Singular Matrix \n",
    "    X1 = data.lai.resample(time='Y', label='left').mean().compute()\n",
    "    X2 = data.ts.resample(time='Y', label='left').mean().compute()\n",
    "    X3 = data.pr.resample(time='Y', label='left').sum().compute()\n",
    "    X4 = data.rsds.resample(time='Y', label='left').mean().compute()\n",
    "    Y = data.hfls.resample(time='Y', label='left').mean().compute()\n",
    "    \n",
    "    X1 = X1.fillna(0.001); X1 = X1.where(X1 != 0, 0.001)\n",
    "    X2 = X2.fillna(0.001); X2 = X2.where(X2 != 0, 0.001)\n",
    "    X3 = X3.fillna(0.001); X3 = X3.where(X3 != 0, 0.001)\n",
    "    X4 = X4.fillna(0.001); X4 = X4.where(X4 != 0, 0.001)\n",
    "    Y = Y.fillna(0.001); Y = Y.where(Y != 0, 0.001)\n",
    "    ## Preprocessing\n",
    "    # Standardization\n",
    "    X1 = X1 - X1.mean(dim = [\"time\"])/X1.std(dim = [\"time\"])\n",
    "    X2 = X2 - X2.mean(dim = [\"time\"])/X2.std(dim = [\"time\"])\n",
    "    X3 = X3 - X3.mean(dim = [\"time\"])/X3.std(dim = [\"time\"])\n",
    "    X4 = X4 - X4.mean(dim = [\"time\"])/X4.std(dim = [\"time\"])\n",
    "\n",
    "    # Gap filling\n",
    "    X1 = X1.fillna(0.001); X1 = X1.where(X1 != 0, 0.001)\n",
    "    X2 = X2.fillna(0.001); X2 = X2.where(X2 != 0, 0.001)\n",
    "    X3 = X3.fillna(0.001); X3 = X3.where(X3 != 0, 0.001)\n",
    "    X4 = X4.fillna(0.001); X4 = X4.where(X4 != 0, 0.001)\n",
    "    Y = Y.fillna(0.001); Y = Y.where(Y != 0, 0.001)\n",
    "\n",
    "    # Stack on 1D vectors\n",
    "    X1 = X1.stack(cell = [\"lon\",\"lat\"]); X2 = X2.stack(cell = [\"lon\",\"lat\"]); X3 = X3.stack(cell = [\"lon\",\"lat\"]); X4 = X4.stack(cell = [\"lon\",\"lat\"]); Y = Y.stack(cell = [\"lon\",\"lat\"])\n",
    "\n",
    "    # Create empty dataarray to store regression diagnostics\n",
    "    xr_coef = xr.DataArray(data=None, coords=[X1.cell], dims=[\"cell\"])\n",
    "    xr_pval = xr.DataArray(data=None, coords=[X1.cell], dims=[\"cell\"])\n",
    "    xr_r2 = xr.DataArray(data=None, coords=[X1.cell], dims=[\"cell\"])\n",
    "    xr_ypred = xr.DataArray(data=None, coords=[X1.time,X1.cell], dims=[\"time\",\"cell\"])\n",
    "    for i,vars in enumerate(variables):\n",
    "        xr_coef[vars] = xr.DataArray(data=None, coords=[X1.cell], dims=[\"cell\"])\n",
    "        xr_pval[vars] = xr.DataArray(data=None, coords=[X1.cell], dims=[\"cell\"])\n",
    "\n",
    "    # Compute the regression for every pair of consecutive years\n",
    "    years = np.arange(0,X1.time.shape[0]-1,1)\n",
    "    coef_m = []; pval_m = []; r2_m = []\n",
    "    for t in years:\n",
    "        X1 = X1.isel(time = [t,t+1]); X2 = X2.isel(time = [t,t+1]); X3 = X3.isel(time = [t,t+1]); X4 = X4.isel(time = [t,t+1])\n",
    "        Y = Y.isel(time = [t,t+1])\n",
    "\n",
    "        for c in X1.cell:\n",
    "            locator = {'cell':c}\n",
    "            \n",
    "            X = np.array((X1.loc[locator].values, X2.loc[locator].values, X3.loc[locator].values, X4.loc[locator].values)); X = X.T\n",
    "\n",
    "            # Regression\n",
    "            model = linear.fit(X, Y.loc[locator].values)\n",
    "            r2 = linear.score(X, Y.loc[locator].values)\n",
    "            ypred = linear.predict(X)\n",
    "            coef = model.coef_\n",
    "            # p_values = pval_calc(X, Y.loc[locator].values, ypred)\n",
    "            p_values = stats.coef_pval(model,X,Y.loc[locator].values)[1:]\n",
    "        \n",
    "            # Store regression diagnostics\n",
    "            for i,vars in enumerate(variables):\n",
    "                xr_coef[vars].loc[locator] = xr.DataArray(data = coef[i]); \n",
    "                xr_pval[vars].loc[locator] = xr.DataArray(data = p_values[i]); \n",
    "            xr_r2.loc[locator] = xr.DataArray(data = r2);\n",
    "            xr_ypred.loc[locator] =  xr.DataArray(data = ypred)\n",
    "     \n",
    "        xr_coef = xr_coef.unstack()\n",
    "        xr_pval = xr_pval.unstack()\n",
    "        xr_r2 = xr_r2.unstack()\n",
    "        xr_ypred = xr_ypred.unstack()\n",
    "    \n",
    "    # Convert object results to np.float64\n",
    "    for i,vars in enumerate(variables):\n",
    "        xr_coef[vars] = xr_coef[vars].astype(np.float64)\n",
    "        xr_pval[vars] = xr_pval[vars].astype(np.float64)\n",
    "    xr_r2 = xr_r2.astype(np.float64)\n",
    "    xr_ypred = xr_ypred.astype(np.float64)\n",
    "        coef_m.append(coef); pval_m.append(pval); r2_m.append(r2)\n",
    "    coef_m = xr.concat(coef_m,dim = \"delta\").mean(dim = \"delta\")\n",
    "    pval_m = xr.concat(pval_m,dim = \"delta\").mean(dim = \"delta\")\n",
    "    r2_m = xr.concat(r2_m,dim = \"delta\").mean(dim = \"delta\")\n",
    "    \n",
    "    coef_reg.append(coef)\n",
    "    pval_reg.append(pval)\n",
    "    r2_reg.append(r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lumip_kernel",
   "language": "python",
   "name": "lumip_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
